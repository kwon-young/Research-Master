
@inproceedings{couasnon_dmos_2001,
	title = {{DMOS} : a generic document recognition method, application to an automatic generator of musical scores, mathematical formulae and table structures recognition systems},
	shorttitle = {couasnon2001dmos},
	doi = {10.1109/ICDAR.2001.953786},
	abstract = {Genericity in structured document recognition is a difficult challenge. We therefore propose a new generic document recognition method, called DMOS (Description and MOdification of Segmentation), that is made up of a new grammatical formalism, called EPF (Enhanced Position Formalism) and an associated parser which is able to introduce context in segmentation. We implement this method to obtain a generator of document recognition systems. This generator can automatically produce new recognition systems. It is only necessary to describe the document with an EPF grammar, which is then simply compiled. In this way, we have developed various recognition systems: one on musical scores, one on mathematical formulae and one on recursive table structures. We have also defined a specific application to damaged military forms of the 19th Century. We have been able to test the generated system on 5,000 of these military forms. This has permitted us to validate the DMOS method on a real-world application},
	booktitle = {Sixth {International} {Conference} on {Document} {Analysis} and {Recognition}, 2001. {Proceedings}},
	author = {Couasnon, B.},
	year = {2001},
	keywords = {automatic musical score generator, compilation, damaged military forms, data structures, DMOS, document image processing, Enhanced Position Formalism, EPF grammatical formalism, generic document recognition method, grammars, history, image recognition, image segmentation, mathematical formulae, mathematics computing, military computing, music, parser, program compilers, recursive table structure recognition, segmentation context, System testing},
	pages = {215--220},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\QV2I9QDJ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\85T3MMQS\\Couasnon - 2001 - DMOS a generic document recognition method, appli.pdf:application/pdf}
}

@incollection{anquetil_symbol_1999,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Symbol} {Classifier} {Able} to {Reject} {Wrong} {Shapes} for {Document} {Recognition} {Systems}},
	copyright = {©2000 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-41222-9 978-3-540-40953-3},
	url = {http://link.springer.com/chapter/10.1007/3-540-40953-X_17},
	abstract = {We propose in this paper a new framework to develop a transparent classifier able to deal with reject notions. The generated classifier can be characterized by a strong reliability without loosing good properties in generalization. We show on a musical scores recognition system that this classifier is very well suited to develop a complete document recognition system. Indeed this classifier allows them firstly to extract known symbols in a document (text for example) and secondly to validate segmentation hypotheses. Tests had been successfully performed on musical and digit symbols databases.},
	language = {en},
	number = {1941},
	urldate = {2015-12-07},
	booktitle = {Graphics {Recognition} {Recent} {Advances}},
	publisher = {Springer Berlin Heidelberg},
	author = {Anquetil, Éric and Coüasnon, Bertrand and Dambreville, Frédéric},
	editor = {Chhabra, Atul K. and Dori, Dov},
	month = sep,
	year = {1999},
	note = {DOI: 10.1007/3-540-40953-X\_17},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Classification, Computer Graphics, Document Recognition, Genetic Algorithm, Image Processing and Computer Vision, Musical Score, Neural networks, pattern recognition, Radial Basis Function, Reliability, Symbol recognition},
	pages = {209--218},
	file = {Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\ZU5KNJSU\\Anquetil et al. - 1999 - A Symbol Classifier Able to Reject Wrong Shapes fo.pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\79SXA4RJ\\3-540-40953-X_17.html:text/html}
}

@inproceedings{couasnon_using_1995,
	title = {Using {Logic} {Programming} {Languages} {For} {Optical} {Music} {Recognition}},
	abstract = {Optical Music Recognition is a particular form of document analysis in which there is much knowledge about document structure. Indeed there exists an important set of rules for musical notation, but current systems do not fully use them. We propose a new solution using a grammar to guide the segmentation of the graphical objects and their recognition. The grammar is essentially a description of the relations (relative position and size, adjacency, etc) between the graphical objects. Inspired by Definite Clause Grammar techniques, the grammar can be directly implemented in Prolog, a higher-order dialect of Prolog. Moreover, the translation from the grammar into Prolog code can be done automatically. Our approach is justified by the first encouraging results obtained with a prototype for music score recognition.  Keywords: Document analysis, Optical Music Recognition, DCG, Grammar Translation  1 Introduction  In structured document analysis, one open problem is to separate knowledge from...},
	booktitle = {In {Proceedings} of the {Third} {International} {Conference} on {The} {Practical} {Application} of {Prolog}},
	author = {Coüasnon, Bertrand and Brisset, Pascal and Stéphan, Igor and Brisset, Couasnon Pascal},
	year = {1995},
	pages = {115--134},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\3CFRWUX7\\Coüasnon et al. - 1995 - Using Logic Programming Languages For Optical Musi.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\ARH3DNW7\\summary.html:text/html}
}

@article{couasnon_using_1994,
	title = {Using grammars to segment and recognize music scores},
	url = {ftp://ftp.idsa.prd.fr/local/IMADOC/couasnon/Articles/das94.ps},
	urldate = {2015-12-07},
	journal = {International Association for Pattern Recognition Workshop on Document Analysis Systems},
	author = {Coüasnon, Bertrand and Camillerapp, Jean},
	year = {1994},
	pages = {15--27},
	file = {dmos[4].pdf:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\NUW2KT6Q\\dmos[4].pdf:application/pdf}
}

@inproceedings{couasnon_way_1995,
	title = {A way to separate knowledge from program in structured document analysis: application to optical music recognition},
	volume = {2},
	shorttitle = {A way to separate knowledge from program in structured document analysis},
	doi = {10.1109/ICDAR.1995.602099},
	abstract = {Optical Music Recognition is a form of document analysis for which a priori knowledge is particularly important. Musical notation is governed by a substantial set of rules, but current systems fail to use them adequately. In complex scores, existing systems cannot overcome the well-known segmentation problems of document analysis, due mainly to the high density of music information. This paper proposes a new method of recognition which uses a grammar in order to formalize the syntactic rules and represent the context. However, where objects touch, there is a discrepancy between the way the existing knowledge (grammar) will describe an object and the way it is recognized, since touching objects have to be segmented first. Following a description of the grammar, this paper shall go on to propose the use of an operator to modify the way the grammar parses the image so that the system can deal with certain touching objects (e.g. where an accidental touches a notehead)},
	booktitle = {, {Proceedings} of the {Third} {International} {Conference} on {Document} {Analysis} and {Recognition}, 1995},
	author = {Couasnon, B. and Camillerapp, J.},
	month = aug,
	year = {1995},
	keywords = {a priori knowledge, character recognition, document analysis, document image processing, grammar, Image analysis, image recognition, image segmentation, Information analysis, Joining processes, Labeling, Multiple signal classification, music, musical notation, Optical music recognition, Particle beam optics, segmentation problems, syntactic rules, Text analysis, touching objects, Ultraviolet sources},
	pages = {1092--1097 vol.2},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\NNBRS6UA\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\FX89ZGPW\\Couasnon et Camillerapp - 1995 - A way to separate knowledge from program in struct.pdf:application/pdf}
}

@article{lemaitre_multiresolution_2008,
	title = {Multiresolution cooperation makes easier document structure recognition},
	volume = {11},
	issn = {1433-2833, 1433-2825},
	url = {http://link.springer.com/article/10.1007/s10032-008-0072-6},
	doi = {10.1007/s10032-008-0072-6},
	abstract = {This paper shows the interest of imitating the perceptive vision to improve the recognition of the structure of ancient, noisy and low structured documents. The perceptive vision, that is used by human eye, consists in focusing attention on interesting elements after having detecting their presence in a global vision process. We propose a generic method in order to apply this concept to various problems and kinds of documents. Thus, we introduce the concept of cooperation between multiresolution visions into a generic method. The originality of this work is that the cooperation between resolutions is totally led by the knowledge dedicated to each kind of document. In this paper, we present this method on three kinds of documents: handwritten low structured mail documents, naturalization decree register that are archive noisy documents from the 19th century and Bangla script that requires a precise vision. This work is validated on 86,291 documents.},
	language = {en},
	number = {2},
	urldate = {2015-12-08},
	journal = {IJDAR},
	author = {Lemaitre, Aurélie and Camillerapp, Jean and Coüasnon, Bertrand},
	month = sep,
	year = {2008},
	keywords = {grammar, Image Processing and Computer Vision, Multiresolution, pattern recognition, Perceptive vision, Structure recognition},
	pages = {97--109},
	file = {Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\6RFT6P84\\Lemaitre et al. - 2008 - Multiresolution cooperation makes easier document .pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\T4PG66VI\\s10032-008-0072-6.html:text/html}
}

@misc{_understanding_????,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2015-11-19},
	file = {Understanding LSTM Networks -- colah's blog:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\2DTWZ5HS\\2015-08-Understanding-LSTMs.html:text/html}
}

@misc{_public:publications_????,
	title = {public:publications [{A}2iALab]},
	url = {http://www.a2ialab.com/doku.php?id=public:publications},
	urldate = {2015-11-23},
	file = {public\:publications [A2iALab]:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\JQZ6R83K\\doku.html:text/html}
}

@inproceedings{moysset_a2ia_2014,
	title = {The {A}2iA {Multi}-lingual {Text} {Recognition} {System} at the {Second} {Maurdor} {Evaluation}},
	doi = {10.1109/ICFHR.2014.57},
	abstract = {This paper describes the system submitted by A2iA to the second Maurdor evaluation for multi-lingual text recognition. A system based on recurrent neural networks and weighted finite state transducers was used both for printed and handwritten recognition, in French, English and Arabic. To cope with the difficulty of the documents, multiple text line segmentations were considered. An automatic procedure was used to prepare annotated text lines needed for the training of the neural network. Language models were used to decode sequences of characters or words for French and English and also sequences of part-of-arabic words (PAWs) in case of Arabic. This system scored first at the second Maurdor evaluation for both printed and handwritten text recognition in French, English and Arabic.},
	booktitle = {2014 14th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Moysset, B. and Bluche, T. and Knibbe, M. and Benzeghiba, M.F. and Messina, R. and Louradour, J. and Kermorvant, C.},
	month = sep,
	year = {2014},
	keywords = {A2iA multilingual text recognition system, annotated text lines, Arabic, document image processing, English, Error analysis, French, handwriting recognition, handwritten recognition, handwritten text recognition, Hidden Markov models, image segmentation, language model, learning (artificial intelligence), Maurdor evaluation, multiple text line segmentation, natural language processing, neural network training, OCR, part-of-arabic words, PAW, printed recognition, printed text recognition, recurrent neural nets, recurrent neural network, text detection, Text recognition, Training, Training data, weighted finite state transducer},
	pages = {297--302},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\4SAPF3ED\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\W37JNNQM\\Moysset et al. - 2014 - The A2iA Multi-lingual Text Recognition System at .pdf:application/pdf}
}

@inproceedings{mnih_recurrent_2014,
	title = {Recurrent models of visual attention},
	url = {http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention},
	urldate = {2016-01-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and {others}},
	year = {2014},
	pages = {2204--2212},
	file = {Recurrent Models of Visual Attention - 5542-recurrent-models-of-visual-attention.pdf:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\QVXTFNVG\\5542-recurrent-models-of-visual-attention.pdf:application/pdf}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, S and Schmidhuber, J},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\DCHH4QEP\\freeabs_all.html:text/html;Long Short-Term Memory.pdf:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\7KQDRK8W\\Long Short-Term Memory.pdf:application/pdf}
}

@inproceedings{graves_connectionist_2006,
	title = {Connectionist temporal classification: {Labelling} unsegmented sequence data with recurrent neural networks},
	shorttitle = {Connectionist temporal classification},
	abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN. 1.},
	booktitle = {In {Proceedings} of the {International} {Conference} on {Machine} {Learning}, {ICML} 2006},
	author = {Graves, Alex and Fernández, Santiago and Gomez, Faustino},
	year = {2006},
	pages = {369--376},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\E7XSTBEF\\Graves et al. - 2006 - Connectionist temporal classification Labelling u.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\Q64BT2IM\\summary.html:text/html}
}

@article{ba_multiple_2014,
	title = {Multiple {Object} {Recognition} with {Visual} {Attention}},
	url = {http://arxiv.org/abs/1412.7755},
	abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
	urldate = {2015-12-07},
	journal = {arXiv:1412.7755 [cs]},
	author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.7755},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1412.7755 PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\582DH3QD\\Ba et al. - 2014 - Multiple Object Recognition with Visual Attention.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\NHRCQN69\\1412.html:text/html}
}

@inproceedings{graves_offline_2009,
	title = {Offline {Handwriting} {Recognition} with {Multidimensional} {Recurrent} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks},
	abstract = {Eletronic Proceedings of Neural Information Processing Systems},
	urldate = {2015-12-07},
	author = {Graves, Alex and Schmidhuber, Juergen},
	year = {2009},
	pages = {545--552},
	file = {Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\NF6BBXNV\\Graves et Schmidhuber - 2009 - Offline Handwriting Recognition with Multidimensio.pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\DEKVIU2E\\3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.html:text/html}
}

@inproceedings{pugin_optical_2006,
	title = {Optical {Music} {Recognitoin} of {Early} {Typographic} {Prints} using {Hidden} {Markov} {Models}.},
	url = {http://www.aruspix.net/publications/pugin06optical.pdf},
	urldate = {2015-12-07},
	booktitle = {{ISMIR}},
	author = {Pugin, Laurent},
	year = {2006},
	pages = {53--56},
	file = {[PDF] à partir de aruspix.net:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\HIQQH8J8\\Pugin - 2006 - Optical Music Recognitoin of Early Typographic Pri.pdf:application/pdf}
}

@article{rebelo_optical_2009,
	title = {Optical recognition of music symbols},
	volume = {13},
	issn = {1433-2833, 1433-2825},
	url = {http://link.springer.com/article/10.1007/s10032-009-0100-1},
	doi = {10.1007/s10032-009-0100-1},
	abstract = {Many musical works produced in the past are still currently available only as original manuscripts or as photocopies. The preservation of these works requires their digitalization and transformation into a machine-readable format. However, and despite the many research activities on optical music recognition (OMR), the results for handwritten musical scores are far from ideal. Each of the proposed methods lays the emphasis on different properties and therefore makes it difficult to evaluate the efficiency of a proposed method. We present in this article a comparative study of several recognition algorithms of music symbols. After a review of the most common procedures used in this context, their respective performances are compared using both real and synthetic scores. The database of scores was augmented with replicas of the existing patterns, transformed according to an elastic deformation technique. Such transformations aim to introduce invariances in the prediction with respect to the known variability in the symbols, particularly relevant on handwritten works. The following study and the adopted databases can constitute a reference scheme for any researcher who wants to confront a new OMR algorithm face to well-known ones.},
	language = {en},
	number = {1},
	urldate = {2015-12-07},
	journal = {IJDAR},
	author = {Rebelo, A. and Capela, G. and Cardoso, Jaime S.},
	month = nov,
	year = {2009},
	keywords = {document image processing, Image Processing and Computer Vision, music, Off-line recognition, pattern recognition, Performance evaluation, Symbol recognition},
	pages = {19--31},
	file = {Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\K72TN5UJ\\Rebelo et al. - 2009 - Optical recognition of music symbols.pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\CGR5BNP8\\s10032-009-0100-1.html:text/html}
}

@inproceedings{erhan_scalable_2014,
	title = {Scalable {Object} {Detection} {Using} {Deep} {Neural} {Networks}},
	doi = {10.1109/CVPR.2014.276},
	abstract = {Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Erhan, D. and Szegedy, C. and Toshev, A. and Anguelov, D.},
	month = jun,
	year = {2014},
	keywords = {Agriculture, class-agnostic bounding boxes, deep convolutional neural networks, Detectors, ILSVRC-2012, ImageNet large-scale visual recognition challenge, image recognition, neural nets, Neural networks, object detection, scalable object detection, Training, Visualization},
	pages = {2155--2162},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\EP5WTX62\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\9ZTEHBRP\\Erhan et al. - 2014 - Scalable Object Detection Using Deep Neural Networ.pdf:application/pdf}
}

@article{messina_segmentation-free_????,
	title = {Segmentation-free {Handwritten} {Chinese} {Text} {Recognition} with {LSTM}-{RNN}},
	url = {http://www.researchgate.net/profile/Ronaldo_Messina/publication/278022925_Segmentation-free_Handwritten_Chinese_Text_Recognition_with_LSTM-RNN/links/55dae60f08aec156b9ae8c79.pdf},
	urldate = {2015-12-07},
	author = {Messina, Ronaldo and Louradour, Jérôme},
	file = {[PDF] à partir de researchgate.net:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\46C25787\\Messina et Louradour - Segmentation-free Handwritten Chinese Text Recogni.pdf:application/pdf}
}

@inproceedings{davila_using_2014,
	title = {Using {Off}-{Line} {Features} and {Synthetic} {Data} for {On}-{Line} {Handwritten} {Math} {Symbol} {Recognition}},
	doi = {10.1109/ICFHR.2014.61},
	abstract = {We present an approach for on-line recognition of handwritten math symbols using adaptations of off-line features and synthetic data generation. We compare the performance of our approach using four different classification methods: AdaBoost. M1 with C4.5 decision trees, Random Forests and Support-Vector Machines with linear and Gaussian kernels. Despite the fact that timing information can be extracted from on-line data, our feature set is based on shape description for greater tolerance to variations of the drawing process. Our main datasets come from the Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME) 2012 and 2013. Class representation bias in CROHME datasets is mitigated by generating samples for underrepresented classes using an elastic distortion model. Our results show that generation of synthetic data for underrepresented classes might lead to improvements of the average per-class accuracy. We also tested our system using the Math Brush dataset achieving a top-1 accuracy of 89.87\% which is comparable with the best results of other recently published approaches on the same dataset.},
	booktitle = {2014 14th {International} {Conference} on {Frontiers} in {Handwriting} {Recognition} ({ICFHR})},
	author = {Davila, K. and Ludi, S. and Zanibbi, R.},
	month = sep,
	year = {2014},
	keywords = {Accuracy, AdaBoost, AdaBoost. M1, C4.5 decision trees, classification methods, Elastic Distortion, feature extraction, feature set extraction, Gaussian kernels, handwritten character recognition, Histograms, image classification, Kernel, learning (artificial intelligence), mathematics computing, off-line features, on-line handwritten math symbol recognition, Random Forest, random forests, Shape, support vector machines, support-vector machines, SVM, synthetic data generation, Testing, Training},
	pages = {323--328},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\37VUX6B3\\abs_all.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\8SX3ECSD\\Davila et al. - 2014 - Using Off-Line Features and Synthetic Data for On-.pdf:application/pdf}
}

@incollection{liwicki_neural_2012,
	series = {Studies in {Computational} {Intelligence}},
	title = {Neural {Networks} for {Handwriting} {Recognition}},
	copyright = {©2012 Springer Berlin Heidelberg},
	isbn = {978-3-642-24048-5 978-3-642-24049-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-24049-2_2},
	abstract = {In this chapter a novel kind of Recurrent Neural Networks (RNNs) is described. Bi- and Multidimensional RNNs combined with Connectionist Temporal Classification allow for a direct recognition of raw stroke data or raw pixel data. In general, recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to assimilate context information, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing, or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This chapter describes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range, bidirectional or multidirectional interdependencies. In experiments on two unconstrained handwriting databases, the new approach achieves word recognition accuracies of 79,7\% on on-line data and 74,1\% on off-line data, significantly outperforming a state-of-the-art HMM-based system. Promising experimental results on various other datasets from different countries are also presented. A toolkit implementing the networks is freely available for public.},
	language = {en},
	number = {386},
	urldate = {2016-01-04},
	booktitle = {Computational {Intelligence} {Paradigms} in {Advanced} {Pattern} {Classification}},
	publisher = {Springer Berlin Heidelberg},
	author = {Liwicki, Marcus and Graves, Alex and Bunke, Horst},
	editor = {Ogiela, Marek R. and Jain, Lakhmi C.},
	year = {2012},
	note = {DOI: 10.1007/978-3-642-24049-2\_2},
	keywords = {Artificial Intelligence (incl. Robotics), Computational Intelligence, pattern recognition},
	pages = {5--24},
	file = {Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\PPG9CPRW\\Liwicki et al. - 2012 - Neural Networks for Handwriting Recognition.pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\K52E6P4X\\978-3-642-24049-2_2.html:text/html}
}

@misc{_maurdor_2016,
	title = {Maurdor},
  howpublished = {\url{http://www.maurdor-campaign.org/index.php?id=52&L=1}},
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1053-587X},
	doi = {10.1109/78.650093},
	abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, Kuldip K.},
	month = nov,
	year = {1997},
	keywords = {artificial data, Artificial neural networks, bidirectional recurrent neural networks, classification experiments, complete symbol sequences, conditional posterior probability, Control systems, Databases, learning by example, learning from examples, negative time direction, Parameter estimation, pattern classification, phonemes, positive time direction, Probability, real data, recurrent neural nets, Recurrent neural networks, regression experiments, regular recurrent neural network, Shape, speech processing, speech recognition, statistical analysis, Telecommunication control, TIMIT database, Training, Training data},
	pages = {2673--2681},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\8ZTXQD8I\\abs_all.html:text/html;Schuster et Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\IES745PZ\\Schuster et Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf}
}

@incollection{fornes_analysis_2014,
	title = {Analysis and {Recognition} of {Music} {Scores}},
	copyright = {©2014 Springer-Verlag London},
	isbn = {978-0-85729-858-4 978-0-85729-859-1},
	url = {http://link.springer.com/referenceworkentry/10.1007/978-0-85729-859-1_24},
	abstract = {The analysis and recognition of music scores has attracted the interest of researchers for decades. Optical Music Recognition (OMR) is a classical research field of Document Image Analysis and Recognition (DIAR), whose aim is to extract information from music scores. Music scores contain both graphical and textual information, and for this reason, techniques are closely related to graphics recognition and text recognition. Since music scores use a particular diagrammatic notation that follow the rules of music theory, many approaches make use of context information to guide the recognition and solve ambiguities. This chapter overviews the main Optical Music Recognition (OMR) approaches. Firstly, the different methods are grouped according to the OMR stages, namely, staff removal, music symbol recognition, and syntactical analysis. Secondly, specific approaches for old and handwritten music scores are reviewed. Finally, online approaches and commercial systems are also commented.},
	language = {en},
	urldate = {2015-12-07},
	booktitle = {Handbook of {Document} {Image} {Processing} and {Recognition}},
	publisher = {Springer London},
	author = {Fornés, Alicia and Sánchez, Gemma},
	editor = {Doermann, David and Tombre, Karl},
	year = {2014},
	note = {DOI: 10.1007/978-0-85729-859-1\_24},
	keywords = {Document Preparation and Text Processing, Graphics recognition, History of Computing, Image Processing and Computer Vision, Language Translation and Linguistics, Optical music recognition, pattern recognition, Staff removal, Symbolrecognition, The Computer Industry},
	pages = {749--774},
	file = {handbook - chap 22 - Music.pdf:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\CW88BSWS\\handbook - chap 22 - Music.pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\IN6EKGXZ\\978-0-85729-859-1_24.html:text/html}
}

@article{rebelo_optical_2012,
	title = {Optical music recognition: state-of-the-art and open issues},
	volume = {1},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Optical music recognition},
	url = {http://link.springer.com/article/10.1007/s13735-012-0004-6},
	doi = {10.1007/s13735-012-0004-6},
	abstract = {For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones.},
	language = {en},
	number = {3},
	urldate = {2015-12-07},
	journal = {Int J Multimed Info Retr},
	author = {Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R. S. and Guedes, Carlos and Cardoso, Jaime S.},
	month = mar,
	year = {2012},
	keywords = {Computer music, Computer Science, general, Data Mining and Knowledge Discovery, Image processing, Image Processing and Computer Vision, Information Storage and Retrieval, Information Systems Applications (incl. Internet), Machine learning, Multimedia Information Systems, Music performance},
	pages = {173--190},
	file = {Full Text PDF:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\6SUKIMF4\\Rebelo et al. - 2012 - Optical music recognition state-of-the-art and op.pdf:application/pdf;Snapshot:C\:\\Users\\Kwon-Young\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\kcllvzk4.default\\zotero\\storage\\H69TAC9C\\s13735-012-0004-6.html:text/html}
}
